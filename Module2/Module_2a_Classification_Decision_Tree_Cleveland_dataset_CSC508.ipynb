{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pleunipennings/CSC508_ML_Biomedicine_Class/blob/main/Module2/Module_2a_Classification_Decision_Tree_Cleveland_dataset_CSC508.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C7-8DKKmCaK"
      },
      "source": [
        "## **Welcome to the Module 2 coding part: Classification Decision Tree!**\n",
        "\n",
        "*This notebook was created by Vaisakh Kusabhadran, Amisha Dhawan, Yuomi Zavaleta, Lorena Benitez (all SFSU students), Lucy Moctezuma (CSUEB student) and Pleuni Pennings (SFSU bio professor).*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#OBJECTIVE OF THIS NOTEBOOK:\n",
        "\n",
        "We are going be working with **heart disease data** from the Cleveland dataset.\n",
        "\n",
        "The Cleveland dataset is actually quite old, and it is sort of a classic dataset to apply machine learning methods. The dataset has a lot of variables that are measured for a few hundred patients, and the goal is to predict whether or not the patient has a heart disease. In this exercise heart disease is classified as such, when at least one vessel has > 50% diameter narrowing.\n",
        "\n",
        "The original data contains several (76) variables, however we will only be using 14 of these for this exercise. We will use the same features as stated in the [UCI website](https://archive.ics.uci.edu/ml/datasets/heart+disease). This website also contains documentation for the other features if you are interested in finding out what other variables were captured.\n",
        "\n",
        "Below we have brief descriptions of what each of the features we are going to use mean. The numbers next to the features are the ones that were used in the original dataset.\n",
        "\n",
        "<ul type = \"square\">\n",
        "<li>#3 Age: age in years</li>\n",
        "<li>#4 Sex: sex (1 = male; 0 = female)</li>\n",
        "<li>#9 Chest_pain_type\n",
        "<ul>\n",
        "<li>Value 1: typical angina\n",
        "<li>Value 2: atypical angina\n",
        "<li>Value 3: non-anginal pain\n",
        "<li>Value 4: asymptomatic</li>\n",
        "</ul>\n",
        "<li>#10 At_rest_bp: resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
        "<li>#12 Cholesterol: serum cholestoral in mg/dl </li>\n",
        "<li>#16 Fast_blood_sug: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)</li>\n",
        "<li>#19 Rest_ecg: resting electrocardiographic results\n",
        "<ul>\n",
        "<li>Value 0: normal\n",
        "<li>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
        "<li>Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria</li>\n",
        "</ul>\n",
        "<li>#32 Maxhr: thalach: maximum heart rate achieved</li>\n",
        "<li>#38 Exer_angina: exang: exercise induced angina (1 = yes; 0 = no)</li>\n",
        "<li>#40 Oldpeak: ST depression induced by exercise relative to rest </li>\n",
        "<li>#41 Slope: the slope of the peak exercise ST segment\n",
        "<ul>\n",
        "<li> Value 1: upsloping</li>\n",
        "<li> Value 2: flat</li>\n",
        "<li> Value 3: downsloping</li>\n",
        "</ul>\n",
        "<li>#44 Ca: number of major vessels (0-3) colored by flourosopy</li>\n",
        "<li>#51 Thal: Thallium or stress test 3 = normal; 6 = fixed defect; 7 = reversable defect. See this\n",
        "<a href=\"https://www.healthline.com/health/thallium-stress-test\">website</a>\n",
        "for more info on the thallium or stress test.\n",
        "</li>\n",
        "<li>#58 Diag: num: diagnosis of heart disease (angiographic disease status)\n",
        "<ul>\n",
        "<li>Value 0: no vessel with 50% diameter narrowing</li>\n",
        "<li>Value 1: one vessel with 50% diameter narrowing</li>\n",
        "<li>Value 2,3,4: 2,3,4 vessels with 50% diameter narrowing</li>\n",
        "</ul>\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "The **goal** of this notebook is to create a classification decision tree model for the Cleveland heart disease dataset. I like decision trees because they are easier to understand than most other machine learning or statistical learning methods.\n",
        "\n",
        "This notebook is written by seven # of steps, your mission is to run each cell by clicking in the arrow:\n",
        "\n",
        "See what happens and answer some questions based on the code"
      ],
      "metadata": {
        "id": "BCWB7JUMq3Vc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WHAT IS A CLASSIFICATION DECISION TREE?\n",
        "\n",
        "\n",
        "\n",
        "You can find more information about Decision Tress Classifier here: [Scikit-learn:DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "\n",
        "There is also information about how decision trees for classification problems are made in the [course text for module 2](https://docs.google.com/document/d/e/2PACX-1vSYg6aDAhIO7eR6dlc2KrjeriK4hr6m-EmusxSq9v69tM-FJrcOLHe632EWifMaNbwP8MszEbpTH6ei/pub)\n"
      ],
      "metadata": {
        "id": "nQ6adxEavsq7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR77ZeZ0_jcF"
      },
      "source": [
        "##**Step 1) Importing packages**\n",
        "\n",
        "Here we importing all the necessary packages â€“ we'll explain what they do later when we use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ykwp-svEYtD"
      },
      "source": [
        "# Importing packages that deal with Data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Importing packages dealing with Machine Learning modeling\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import tree\n",
        "import graphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45v61Mhu_qSL"
      },
      "source": [
        "##**Step 2) Importing Cleveland dataset**\n",
        "First, we fetch the dataset from the 408 class github repository and state what columns we are interested in. The data are stored in a pandas data frame called \"data\". If you have not heard of the pandas package, it is worth looking it up!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_C_KDqJE-3r"
      },
      "source": [
        "# creating a list of all the feature names we are interested in.\n",
        "columns = [\"Age\",\"Sex\",\"Chest_pain_type\",\"At_rest_bp\",\"Cholesterol\",\"Fast_blood_sug\",\"Rest_ecg\",\"Maxhr\",\"Exer_angina\",\"Oldpeak\",\"Slope\",\"Ca\",\"Thal\",\"Diag\"]\n",
        "\n",
        "# Load the data from class github repository\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/pleunipennings/CSC508Data/main/processed.cleveland.data.txt',header=None, names=columns)\n",
        "# Take a peak of the first 5 rows of our datasert\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbJDugMFAgDQ"
      },
      "source": [
        "Let's look at the data shape. The shape of the data indicates the number of rows and number of columns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nvg634T1pfx"
      },
      "source": [
        "# Lets look at the shape of the data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1:** Looking at code output from the \"data\" (Questions 1-5)"
      ],
      "metadata": {
        "id": "IOK-QlPgNE3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1:**   \n",
        "\n",
        "Each row has data for one patient. How many patients are there in the dataset?\n",
        "\n",
        "Hint: how many rows are there in the data?\n",
        "\n",
        "- *To answer in the cell below, double click on the cell and edit*"
      ],
      "metadata": {
        "id": "FudaSb_mAc1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 answer:**"
      ],
      "metadata": {
        "id": "r9thrJw4BBDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iSA2Eio7ycZ"
      },
      "source": [
        "##**Step 3) Dealing with missing data**\n",
        "Next, we are going to do some work to deal with the missing data. Note that every dataset encodes **Missing values** differently, which is why it is a good idea sometimes to check exactly how missing values were encoded either from your data source, looking at the documentation about the data, etc.\n",
        "\n",
        "In this case our dataset missing values, were encoded with a **\"question mark (?)\"**. Identify the columns having missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNIsc0-7ycZ"
      },
      "source": [
        "# this code adds from each feature column the amount of \"?\" it contains\n",
        "(data=='?').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9TQpKBx8LiS"
      },
      "source": [
        "Here we will display the rows that have missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nIvEbnA8LiT"
      },
      "source": [
        "# looking at the missing values location\n",
        "missing_values = data.loc[(data['Thal'] == \"?\") | (data['Ca'] == \"?\")]\n",
        "missing_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 2:**   \n",
        "\n",
        "Were there any features with missing data, before we dealt with them?\n",
        "\n",
        "Which columns contained missing values and how many missing values were in each feature?"
      ],
      "metadata": {
        "id": "OoZGt_xlD2LT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2 answer:**"
      ],
      "metadata": {
        "id": "9GyXgHnbD2LT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "Before we started to deal with missing data, let's stop and examine another key feature in data\n",
        "handling: There are some models that can only take \"numbers\" for\n",
        "instance, and others that can take both. There are also situations where we might need to encode a categorical variable differently.\n",
        "\n",
        "Use **dtypes()** function to look at the data types."
      ],
      "metadata": {
        "id": "TaGbyGk_9BnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the data type for each of the features\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "dW7UXq349Vbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3:**\n",
        "\n",
        "Look at the datatypes of all columns. Why do you think \"Ca\" and \"Thal\" have datatype \"object\" whereas all the others have float64 or int64?"
      ],
      "metadata": {
        "id": "-hlu5G0mEIZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 answer:**"
      ],
      "metadata": {
        "id": "EjkyZQh4EIZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a closer examination into these different \"Ca\" and \"Thal\" variables."
      ],
      "metadata": {
        "id": "MqwzJ8MtEPUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print the unique values of these variables\n",
        "print(data[\"Thal\"].unique())\n",
        "print(data[\"Ca\"].unique())"
      ],
      "metadata": {
        "id": "BMEciqF1EPUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to always check if any columns are strange. In some cases we will see there are both numeric values and character values, like 0.0 and \"0.0\". In our case, one approach is to make all of them into strings, since there's no real numeric meaning for these values and they are taken as categories."
      ],
      "metadata": {
        "id": "CQeKf6gEEPUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Data types and checking again\n",
        "data[\"Thal\"] = data[\"Thal\"].astype(str)\n",
        "data[\"Ca\"] = data[\"Ca\"].astype(str)\n",
        "print(data[\"Thal\"].unique())\n",
        "print(data[\"Ca\"].unique())"
      ],
      "metadata": {
        "id": "pyxWWNrEEPUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "Now, let's deal with the missing data.\n",
        "\n",
        "There are different ways to deal with missing data. Some ways to\n",
        "deal with it could be replacing it with the mean, eliminating it from the data set, etc. Here we will replace the missing values with the median of the column.\n"
      ],
      "metadata": {
        "id": "bCLixIUoLM4a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljNYxQ278lCe"
      },
      "source": [
        "# Replace the missing values(?) with nan.\n",
        "data = data.replace('?', np.nan)\n",
        "\n",
        "#====================================================#\n",
        "# Convert 'Thal' and 'Ca' columns to numeric before calculating the median\n",
        "data['Thal'] = pd.to_numeric(data['Thal'])\n",
        "data['Ca'] = pd.to_numeric(data['Ca'])\n",
        "#====================================================#\n",
        "\n",
        "#  Replace nan with the median for the columns.\n",
        "data['Thal'] = data['Thal'].fillna(data['Thal'].median())\n",
        "data['Ca'] = data['Ca'].fillna(data['Ca'].median())\n",
        "\n",
        "# verify that there are no missing values\n",
        "(data==np.nan).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print the unique values of these variables\n",
        "print(data[\"Thal\"].unique())\n",
        "print(data[\"Ca\"].unique())"
      ],
      "metadata": {
        "id": "tAOarDU3dT8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4a7SpuY7sD7"
      },
      "source": [
        "The value_counts() function tells us how often each value occurs. In the example below we can see that our current Target variable \"Diag\" (Diagnosis) has 4 levels, as described earlier. We will be cleaning this up by collapsing 1 through 4 into a single level (1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcFW-PeV6wTn"
      },
      "source": [
        "# Check how many people have each type of diagnosis\n",
        "data[\"Diag\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 4:**\n",
        "\n",
        "The \"Diag\" column holds the actual diagnosis for the patient. 0 means that had no vessels that were > 50% constricted. 1, 2, 3, 4 means they had 1, 2, 3, or 4 constricted vessels. How many patients had at least one constricted vessel?"
      ],
      "metadata": {
        "id": "kbOJ9Ct2ETeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4 answer:**"
      ],
      "metadata": {
        "id": "nclrWqBGEiHv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRpdAqWS7jyj"
      },
      "source": [
        "The describe() function gives you a summary of the descriptive statistics of the dataframe. Notice that \"Ca\" and \"Thal\" will not be here if they are in object type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJcOBkKc5WbT"
      },
      "source": [
        "# Lets look at some of the descriptive statistics of our dataset.\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pairplot:** is a matrix of scatterplots that allows us to check how each of the column variables in our dataset correlate to each other.\n",
        "\n",
        "This plot can look overwhelming at first but picture slicing this giant square through the diagonal! Notice that you only need to pay attention to either the lower OR upper triangle of this giant plot, because the other half is repeated information, except the x and y coordinates of each scatterplot get flipped.\n",
        "\n",
        "We will be using the pairplot on the seaborn package (sns), The diagonal contains histograms for each variable."
      ],
      "metadata": {
        "id": "ZaF5tM9dPKqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 5:**\n",
        "\n",
        "Look at the sns pairplot. Do you see any variable that is clearly correlated with age? If so, which?"
      ],
      "metadata": {
        "id": "1pH30R86EuBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5 answer:**"
      ],
      "metadata": {
        "id": "YCD_BP2OFgmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at the pairplot of our dataset.\n",
        "g = sns.pairplot(data)\n",
        "g.fig.set_size_inches(15,15)"
      ],
      "metadata": {
        "id": "SVJTdqWOPe55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0NmCQRT9l6x"
      },
      "source": [
        "##**Step 4) Split target and features**\n",
        "Next, we want to take the \"Diag\" column out of the features dataframe, because it is actually not a feature (in our analysis) but it is the \"target\", the thing we want to predict which is the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NQ2UNjJwboV"
      },
      "source": [
        "# stablish that all the other columns are considered features except \"Diag\"\n",
        "features = data.drop(columns='Diag')\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIMYd1id4NCS"
      },
      "source": [
        "# stablish that Diag is your label\n",
        "labels = np.array(data[\"Diag\"])\n",
        "\n",
        "#store labels as variable y\n",
        "y = labels\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnoHyQOFvNNy"
      },
      "source": [
        "Next, we are converting the labels (stored in variable y) to binary values so that the model is trained just to predict the presence/absence of heart disease. All the values that are 1 or higher will just be 1 from here on. The output below shows that all the other values (2,3 and 4) are now all number 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE0eZtynrvpq"
      },
      "source": [
        "# storing all the values higher than 1 as just 1\n",
        "y = np.where(y >= 1,1,0)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpF5mwwc_yUJ"
      },
      "source": [
        "We can now use the np.count_nonzero() function to count how many people there are with and without heart disease in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPc0_Cfj_1x_"
      },
      "source": [
        "# Printing the amount of people with and without heart disease\n",
        "print(\"number patients with no heart disease = \" + str(np.count_nonzero(y==0)))\n",
        "print(\"number patients with heart disease = \" + str(np.count_nonzero(y==1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6PZxP1Bk96"
      },
      "source": [
        "##**Step 5) Separating train and test data**\n",
        "\n",
        "In machine learning, it is common to split a dataset in two. A larger chunk will be the training data and a smaller chunk will be the test data. The idea is that you build a predictive model with the training data and then you use the test data to see if your model is any good. Here we will do the splitting, using a function called train_test_split from scikit-learn.\n",
        "\n",
        "* **test_size** = 0.3. Gives the proportion of the dataset to include in the test set. 0.3 represents 30%. <br>\n",
        "* **random_state** = 2. Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits. The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order and therefore you get exactly the same results when you re-run the code and you also get the same results as someone else in the class. More details - [Random_state](https://scikit-learn.org/stable/glossary.html#term-random_state)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfXuHyXuFKUn"
      },
      "source": [
        "# this splits the data into 4 different parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.3, random_state=2) # 70% training and 30% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEMZpvczEweG"
      },
      "source": [
        "# lets see only the features that will be used for training\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Task 2:** keeping the overview (Questions 6-7)"
      ],
      "metadata": {
        "id": "SX3T2WJaSzz0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzMYdHxRo682"
      },
      "source": [
        "### **Question 6:**\n",
        "\n",
        "Write a list of some of the functions we have used from numpy (np), pandas (pd) and scikit-learn (sklearn) and explain the use of the functions from these packages.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numpy (np) function list:**\n",
        "\n",
        "\n",
        "\n",
        "1.   list item\n",
        "2.   list item\n",
        "3.   list item\n",
        "4.   list item\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ykLv1CATRPV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas (pd) function list:**\n",
        "\n",
        "\n",
        "\n",
        "1.   list item\n",
        "2.   list item\n",
        "3.   list item\n",
        "4.   list item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7R8IMBAIRl9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit-learn (sklearn) function list:**\n",
        "\n",
        "1.   list item\n",
        "2.   list item\n",
        "3.   list item\n",
        "4.   list item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1ScMz4-Rm8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7:**\n",
        "\n",
        "Write a list of all the steps we have taken to get the data ready for making a decision tree.\n",
        "\n",
        "*Hint: steps 2-5*"
      ],
      "metadata": {
        "id": "3IVKS7VnRHhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to get data ready list:**\n",
        "\n",
        "1.   list item\n",
        "2.   list item\n",
        "3.   list item\n",
        "4.   list item\n"
      ],
      "metadata": {
        "id": "hPvNgchsURzV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDtyO4uOrZid"
      },
      "source": [
        "## **Step 6) Making the decision tree and making predictions with it**\n",
        "Here we finally make the decision tree. It is done in two steps. First we create a classifier object (no worries if this doesn't make much sense) and second we train the classifier. We use the training data set (**X_train** and **y_train**) to train the classifier / to make the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4A3mvK8FMmR"
      },
      "source": [
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier(random_state=2)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSAbC0JdMOHa"
      },
      "source": [
        "The decision tree we have created and fitted in the previous code is currently hidden, but we can visualize it, to have a better idea of the underlying decisions it's making in order to end up with the predictions it does."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to visualize our Decision Tree\n",
        "dot_data = tree.export_graphviz(clf, out_file=None,\n",
        "                                  feature_names=features.columns,\n",
        "                                  filled=True, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth= 2)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"Classification tree\")\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "hwoMSMHVTznn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1DJn3DL3jDO"
      },
      "source": [
        "\n",
        "\n",
        "Each node contains 5 pieces of information:\n",
        "- **Conditional Statement**: this determines how the sample is distributed. The first node of our decision tree looks at the Thal (Thallium test). If the patient has a value less than 4.5 (that is the test value was 3, normal) then we go to the left of the tree. If the value was higher than 4.5, we go to the right of the tree.\n",
        "\n",
        "- **Gini index value**: The impurity value calculated for each feature, and each level to decide what node we should use next. For example gini values were calculated for each of the features with a sample of 212 observations and Thal obtained the lowest gini, so it became the root. Afterwards, for each side of the split, gini values were calculated for all features again, this time with a sample of 118 (left side) and a sample of 94 (right side), and the lowest gini from both sides remain Ca and chest pain type, respectively. Gini values will continue to be calculated at each level until it reaches a 0 for a particular branch.\n",
        "\n",
        "- **Samples**: number of observations present at this point in each particular node. The closer we get to the leaf nodes, the smaller the sample sizes.\n",
        "\n",
        "- **values**: an array of 2 values, because we have 2 classes only,each position indicates the current class it belongs to. When the gini value reaches to 0 in a particular node, you should see that one of the values in the 2 number array reaches to 0.\n",
        "\n",
        " **[(0 heart vessels with 50% constriction),( > than 1 vessel with 50% constriction)]**\n",
        "\n",
        "**NOTE**: Our tree is only displaying the top part of our tree. If you would like to see the entire tree, change \"*max_deph*\" argument to 9.\n",
        "\n",
        "Here I decided to plot the distribution of Thal values for the different Diag values. What do you notice? Do you agree with the decision tree that the Thallium test may be a good way to start triaging of patients?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "sns.countplot(data=data, x=\"Diag\", hue=\"Thal\")\n",
        "plt.legend([\"6.0 = Fixed Defect\",\"3.0 = Normal\",\"7.0 = Reversable Defect\"],loc=\"center right\", bbox_to_anchor=(0.85, 0.5))"
      ],
      "metadata": {
        "id": "9gqYV9uCQcN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8DLQeKHxUay"
      },
      "source": [
        "## **Step 7) Looking at the results and plotting the confusion matrix**\n",
        "\n",
        "First, I want to compare the **y_test** array (with the real labels for the 91 patients that are our test dataset) with **y_pred** (with the model-predicted values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivkUrSJDruzn"
      },
      "source": [
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT2gjRuLFN50"
      },
      "source": [
        "# Look at the predicted values. Remember, 0 means no constricted vessels, 1 means at least one.\n",
        "print(y_pred)\n",
        "# And the real values.\n",
        "print(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQhm70PHxfm7"
      },
      "source": [
        "# This is a simple way to look at the number of times y_test is 1 and y_pred is 1 as well (true positives)\n",
        "print(np.count_nonzero(np.logical_and(y_test == 1, y_pred == 1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtde6d7Txb2-"
      },
      "source": [
        "The confusion matrix is a nice way to look at the true and false positive and negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZi4zrExdIv"
      },
      "source": [
        "# Showing the confusin matrix for our Decision tree results\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "plt2 = metrics.ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\n",
        "plt.grid(visible=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WKy4sIWLnmY"
      },
      "source": [
        "Here is a way to calculate the accuracy of the model and summarize it in one value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9rp0imFRBs"
      },
      "source": [
        "acc = round(100 * metrics.accuracy_score(y_test, y_pred),2)\n",
        "print(\"Accuracy:\",acc,\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAwWeB8VY9bM"
      },
      "source": [
        "###**Task 3:** false and true positives and negatives (Questions 8-11)\n",
        "\n",
        "Have a look at this [video](https://www.youtube.com/watch?v=Ivc8c9ijWIQ).\n",
        "Now look at your confusion matrix.\n",
        "\n",
        "###**Question 8:**\n",
        "\n",
        "How many true and false positives and negatives did your model generate?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8 answer:**"
      ],
      "metadata": {
        "id": "h-Fyl2ir0DFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 9:**\n",
        "\n",
        "Use code with np.count_nonzero and np.logical_and to determine the number of false positives."
      ],
      "metadata": {
        "id": "YEUHuNN50V7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run code here:"
      ],
      "metadata": {
        "id": "4HkdK2PZ1G5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9 answer:**"
      ],
      "metadata": {
        "id": "KcuNNZTD0tfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 10:**\n",
        "\n",
        "For what percentage of patients did you predict that they were fine, but really they did have a constricted vessel?\n",
        "\n",
        "Are these false positives or false negatives?"
      ],
      "metadata": {
        "id": "LXWEB16A1V12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10 answer:**"
      ],
      "metadata": {
        "id": "sZ_ghs2M1i8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 11:**\n",
        "\n",
        "Look at the class [Module 2 class notes](https://docs.google.com/document/d/1gguMztbc0MfowLnMl57Um0nJp3_VO6RoI6WGmYDLl78/edit#) to learn about safety and efficiency â€“ calculate safety and efficiency for the decision tree. Include a picture of your confusion matrix (may not be the same as for others)."
      ],
      "metadata": {
        "id": "aMnZ7Js81t45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11 answer:**"
      ],
      "metadata": {
        "id": "IucgTjFQ2jgQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6K1Nzki68Tt"
      },
      "source": [
        "###**Task 4** - final thoughts (Questions 12-16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 12:**\n",
        "\n",
        "Look at the print of the decision tree. What is the second node on the tree on the left and on the right?"
      ],
      "metadata": {
        "id": "478vuPXb4g4Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnfZ1ZRzxrTO"
      },
      "source": [
        "# Graphing Decision Tree Classifer\n",
        "dot_data2 = tree.export_graphviz(clf, out_file=None,\n",
        "                                  feature_names=features.columns,\n",
        "                                  filled=True, rounded=True,\n",
        "                                  special_characters=True,\n",
        "                                  max_depth=4)\n",
        "graph = graphviz.Source(dot_data2)\n",
        "graph.render(\"Classification tree\")\n",
        "display(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 13:**\n",
        "\n",
        "Why do you think the second node left and the second node on the right are not the same?"
      ],
      "metadata": {
        "id": "eQ08334f4jWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 13 answer:**"
      ],
      "metadata": {
        "id": "0k9dTOkjOn6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 14:**\n",
        "\n",
        "Look at some of the nodes that contain age as a variable. What do you notice? Did you expect to see that?"
      ],
      "metadata": {
        "id": "vqdKceoC4mdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 14 answer:**"
      ],
      "metadata": {
        "id": "IQA3vlx1Os_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 15:**\n",
        "\n",
        "Do you think the decision tree we created would be useful for doctors in the ER? Think of one reason why it would be and one reason why it wouldn't be useful."
      ],
      "metadata": {
        "id": "BU7WIt0J4wv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 15 answer:**"
      ],
      "metadata": {
        "id": "B0lHb6wPOw3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Question 16:**\n",
        "\n",
        "Write down one thing you learned today and one thing that confuses you. (I know, I ask this all the time, but it is important to think about it for a few seconds!)"
      ],
      "metadata": {
        "id": "v9-jyFVJ496R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 16 answer:**\n",
        "Learned\n",
        "1.\n",
        "\n",
        "\n",
        "Confused   \n",
        "2.  \n",
        "\n"
      ],
      "metadata": {
        "id": "D6NQTlguO6GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ™‚ Congratulations! You are on your way to mastering machine learning. Give yourself a pat on the back and be proud (even if you feel like some of this is still confusing!)."
      ],
      "metadata": {
        "id": "1UsrBTAk5buA"
      }
    }
  ]
}