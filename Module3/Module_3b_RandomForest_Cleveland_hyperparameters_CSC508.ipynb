{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pleunipennings/CSC508_ML_Biomedicine_Class/blob/main/Module3/Module_3b_RandomForest_Cleveland_hyperparameters_CSC508.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_rsX-Ir7xhK"
      },
      "source": [
        "## **Welcome to the module 3b coding part: Random Forest and hyperparameters!**\n",
        "\n",
        "Whenever you create and train a machine learning model, you need to decide on hyperparameters. Hyperparameters are the settings of the model, set by you (or the defaults programmed by someone else). For example, for a decision tree, you need to decide on the maximum depth of the tree, and for a random forest you need to decide on how many trees you will create. It’s always a good idea to vary these parameters to find the best value of the hyperparameters for your model and your dataset. We do this in a process that we call “hyperparameter tuning”. During hyperparameter tuning, we may create a random forest many times, with a range of numbers of trees and then we determine how many trees are needed to get the best accuracy, without taking up too much computing power. We will practice hyperparameter tuning for the random forest model for the Cleveland dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#DATA FOR THIS NOTEBOOK:\n",
        "\n",
        "We are going to continue working with **heart disease data** from the Cleveland dataset.\n",
        "\n",
        "Below we have brief descriptions of what each of the features we are going to use mean. The numbers next to the features are the ones that were used in the original dataset.\n",
        "\n",
        "<ul type = \"square\">\n",
        "<li>#3 Age: age in years</li>\n",
        "<li>#4 Sex: sex (1 = male; 0 = female)</li>\n",
        "<li>#9 Chest_pain_type\n",
        "<ul>\n",
        "<li>Value 1: typical angina\n",
        "<li>Value 2: atypical angina\n",
        "<li>Value 3: non-anginal pain\n",
        "<li>Value 4: asymptomatic</li>\n",
        "</ul>\n",
        "<li>#10 At_rest_bp: resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
        "<li>#12 Cholesterol: serum cholestoral in mg/dl </li>\n",
        "<li>#16 Fast_blood_sug: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)</li>\n",
        "<li>#19 Rest_ecg: resting electrocardiographic results\n",
        "<ul>\n",
        "<li>Value 0: normal\n",
        "<li>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
        "<li>Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria</li>\n",
        "</ul>\n",
        "<li>#32 Maxhr: thalach: maximum heart rate achieved</li>\n",
        "<li>#38 Exer_angina: exang: exercise induced angina (1 = yes; 0 = no)</li>\n",
        "<li>#40 Oldpeak: ST depression induced by exercise relative to rest </li>\n",
        "<li>#41 Slope: the slope of the peak exercise ST segment\n",
        "<ul>\n",
        "<li> Value 1: upsloping</li>\n",
        "<li> Value 2: flat</li>\n",
        "<li> Value 3: downsloping</li>\n",
        "</ul>\n",
        "<li>#44 Ca: number of major vessels (0-3) colored by flourosopy</li>\n",
        "<li>#51 Thal: Thallium or stress test 3 = normal; 6 = fixed defect; 7 = reversable defect. See this\n",
        "<a href=\"https://www.healthline.com/health/thallium-stress-test\">website</a>\n",
        "for more info on the thallium or stress test.\n",
        "</li>\n",
        "<li>#58 Diag: num: diagnosis of heart disease (angiographic disease status)\n",
        "<ul>\n",
        "<li>Value 0: no vessel with 50% diameter narrowing</li>\n",
        "<li>Value 1: one vessel with 50% diameter narrowing</li>\n",
        "<li>Value 2,3,4: 2,3,4 vessels with 50% diameter narrowing</li>\n",
        "</ul>\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "The **goal** of this notebook is to create a classification decision tree model for the Cleveland heart disease dataset. I like decision trees because they are easier to understand than most other machine learning or statistical learning methods.\n",
        "\n",
        "This notebook is written by seven # of steps, your mission is to run each cell by clicking in the arrow:\n",
        "\n",
        "See what happens and answer some questions based on the code"
      ],
      "metadata": {
        "id": "BCWB7JUMq3Vc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1v9MGH25h_K"
      },
      "source": [
        "# importing packages to handle data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# importing packages to create and evaluate our model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# importing packages to plot\n",
        "from plotnine import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfHd_SKo-9oy"
      },
      "source": [
        "Next, We will be reading from the github repository and cleaning up the dataset as the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xyLXa-4sBA"
      },
      "source": [
        "# loading Dataset from github repository\n",
        "columns = [\"Age\",\"Sex\",\"Chest_pain_type\",\"At_rest_bp\",\"Cholesterol\",\"Fast_blood_sug\",\"Rest_ecg\",\"Maxhr\",\"Exer_angina\",\"Oldpeak\",\"Slope\",\"Ca\",\"Thal\",\"Diag\"]\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/pleunipennings/CSC508Data/main/processed.cleveland.data.txt',header=None, names=columns)\n",
        "\n",
        "# Replacing missing values from dataset with the median of columns\n",
        "data = data.replace('?', np.nan)\n",
        "#====================================================#\n",
        "# Convert 'Thal' and 'Ca' columns to numeric before calculating the median\n",
        "data['Thal'] = pd.to_numeric(data['Thal'])\n",
        "data['Ca'] = pd.to_numeric(data['Ca'])\n",
        "#====================================================#\n",
        "data['Thal'] = data['Thal'].fillna(data['Thal'].median())\n",
        "data['Ca'] = data['Ca'].fillna(data['Ca'].median())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIgKTDmC_sBj"
      },
      "source": [
        "# Splitting labels from features and training from test data\n",
        "**a)** Seperating out the labels (which is what we want to predict, the \"Diag\" column) from the rest of the dataset (the features).\n",
        "\n",
        "Converting the labels to binary values so that the model is trained just to predict the presence/absence of heart disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPtNNPXz7pl4"
      },
      "source": [
        "# Choosing the labels and converting responses into a binary (0) and (1)\n",
        "labels = np.array(data[\"Diag\"])\n",
        "labels = np.where(labels >= 1,1,0)\n",
        "\n",
        "# Specifying features by dropping the labels column and converting it to an array\n",
        "features = data.drop(columns='Diag')\n",
        "features = np.array(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6PZxP1Bk96"
      },
      "source": [
        "**b)** Separating our dataset into training and testing data chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj-yNr3D8XKh"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU5rlOYuB3-J"
      },
      "source": [
        "#Training the Random Forest Model.\n",
        "\n",
        "Before we work on tuning our Random Forest Model, let's first try to check how it performs with all the default parameters. The default settings can be found in the following [sklearn site](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Get used to looking up documentation as it is extremely helpful to always know what hyperparameters are the default, that way we can change them depending on our specific needs.\n",
        "\n",
        "**NOTE:** We will be adding a random seed in this case just to illustrate the same results and to be able to compare these results with the ones after performing Hyperparameter tunning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cutyhw1V8cD8"
      },
      "source": [
        "# Creating Random Forest Model with default parameters\n",
        "rf = RandomForestClassifier(random_state=2)\n",
        "\n",
        "# Fitting/ Training the Random Forest Model as it is\n",
        "rf.fit(train_features, train_labels)\n",
        "\n",
        "# making predictions with model at default settings\n",
        "predictions = rf.predict(test_features)\n",
        "\n",
        "# Calculating Accuracy\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(round(accuracy,2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning one hyper parameter\n",
        "\n",
        "Let's say you want to determine the optimal number of trees in your random forest. Remember that the number of trees is determined by the hyperparameter n_estimators that is passed to the RandomForestClassifier() function.\n",
        "One way to determine what is a good number for n_estimators is to train the random forest model many times while varying the value for n_estimators. We'll do this here with a simple for loop, where we pick a range of numbers for n_estimators by hand ([1, 2, 3, 4, 5, 10, 20, 40, 60, 100,  200, 500]).\n"
      ],
      "metadata": {
        "id": "qmyHvxYK6EYX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zbU0J5G3h-"
      },
      "source": [
        "for numtrees in [1,2,3,4,5,10,20,40,60,100,200,500]:\n",
        "#numtrees = 50\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 2)\n",
        "  rf = RandomForestClassifier(n_estimators=numtrees, random_state = 2) #changing the number of trees\n",
        "  rf.fit(train_features, train_labels);\n",
        "  predictions = rf.predict(test_features)\n",
        "  accuracy = accuracy_score(test_labels, predictions)\n",
        "  print(\"Number of trees:\",numtrees,  \"; accuracy = \", 100* round(accuracy,2), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** that when you remove the random_state parameters, you get a lot more variation between the runs:"
      ],
      "metadata": {
        "id": "Sff251GSe-gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for numtrees in [1, 2, 3, 4, 5, 10, 20, 40, 60, 100,  200, 500]:\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25)\n",
        "  rf = RandomForestClassifier(n_estimators=numtrees) #changing the number of trees\n",
        "  rf.fit(train_features, train_labels);\n",
        "  predictions = rf.predict(test_features)\n",
        "  accuracy = accuracy_score(test_labels, predictions)\n",
        "  print(\"Number of trees:\",numtrees,  \"; accuracy = \", 100* round(accuracy,2), \"%\")"
      ],
      "metadata": {
        "id": "JdRhoZVKyvnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8dJV5Fbt1Ki"
      },
      "source": [
        "## Task 1\n",
        "If you use a different random state, will you get different answers?\n",
        "What if you and one other member of your team run the code with a different random state, or the same?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v61Vnu1asFl"
      },
      "source": [
        "##How does accuracy depend on the other hyper parameters?  \n",
        "\n",
        "Let's see if there is an effect of the choice of the **criterion** for making the nodes of the trees. In the lecture we have seen how gini values are calculated, but there are other criterias besides gini index, used to make our tree nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z22vn2Usaqw7"
      },
      "source": [
        "for criterion in [\"gini\", \"entropy\", \"log_loss\"]:\n",
        "  numtrees = 60\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 2)\n",
        "  rf = RandomForestClassifier(n_estimators=numtrees, random_state = 2, criterion = criterion) #changing the number of trees\n",
        "  rf.fit(train_features, train_labels);\n",
        "  predictions = rf.predict(test_features)\n",
        "  accuracy = accuracy_score(test_labels, predictions)\n",
        "  print(\"Criterion:\",criterion,  \"; accuracy = \", 100* round(accuracy,2), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also vary the **maximum number of features to use**, such as using only chest pain or using chest pain and thal exam results, etc."
      ],
      "metadata": {
        "id": "DwAjgLkT8bzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for max_features in [1, 5, 10, 14]:\n",
        "  numtrees = 60\n",
        "  train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 2)\n",
        "  rf = RandomForestClassifier(n_estimators=numtrees, random_state = 2, max_features = max_features) #changing the number of trees\n",
        "  rf.fit(train_features, train_labels);\n",
        "  predictions = rf.predict(test_features)\n",
        "  accuracy = accuracy_score(test_labels, predictions)\n",
        "  print(\"max_features:\",max_features,  \"; accuracy = \", 100* round(accuracy,2), \"%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0R2SLzjFwOfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9Os5MZUQDM"
      },
      "source": [
        "## Task 2\n",
        "Try writing your own for loop using the hyperparameter **max_leaf_nodes** and see how accuracy changes with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1GHeJBPs_X-"
      },
      "source": [
        "## Let's have a look at two hyper parameters at the same time in a nested loop.\n",
        "The Hyperparameter tunning done above deals only with one parameter at a time, however we can also perform tunning for 2 or more Hyperparameters at the same time and check how it affects accuracy.\n",
        "In our specific case we are testing maxdepths from 1 to 10, and we are using between 1 and all 13 features. So we should have a total of 10 x 13 = 130 possible combination of hyperparameters, thus we are training our random forest 130 times!\n",
        "\n",
        "**NOTE:** This code will take a bit longer to run because it is essentially creating a combination of all possible 2 hyperparameters and then training our random forest with each combination. There are cases where there are even more hyperparameters and data to train, some of my colleagues have told me that it can take up to days and even months to tune!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3ifRWVLgupn"
      },
      "source": [
        "# For this code we have chosen to fix the number of trees\n",
        "num_trees = 200\n",
        "accuracylist = []\n",
        "num_features_list=[]\n",
        "max_depthlist = []\n",
        "\n",
        "for max_depth in range(1,11): # Testing max depths of 1 to 10\n",
        "  for num_features in range(1,train_features.shape[1]+1): # testing number of features from 1 to all features available from training data\n",
        "      rf = RandomForestClassifier(n_estimators = num_trees, max_features = num_features, max_depth = max_depth, bootstrap = True, random_state = 2) # making decision tree object\n",
        "      rf.fit(train_features, train_labels); # train random forest\n",
        "      labels_pred = rf.predict(test_features) # make predictions\n",
        "      accuracy = accuracy_score(test_labels, labels_pred) # get accuracy scores\n",
        "      num_features_list.append(num_features) # appending a number of features into our list named \"num_features_list\"\n",
        "      max_depthlist.append(max_depth) # appending the max depth into our list named \"max_depthlist\"\n",
        "      accuracylist.append(round(accuracy,2)) # appending accuracy into our list named \"accuracylist\"\n",
        "\n",
        "data = {'Num_features': num_features_list, \"MaxDepth\" : max_depthlist, 'Accuracy': accuracylist}\n",
        "# Create DataFrame with all\n",
        "df_features_depth = pd.DataFrame(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can visualize how 2 hyperparameters affect our accuracy. The lighter the color , the higher the accuracies."
      ],
      "metadata": {
        "id": "OJRulDWoWb6A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVRjuVoChuDq"
      },
      "source": [
        "# this code shows us a matrix of accuracies when tunning 2 hyperparameters at the same time\n",
        "(ggplot(df_features_depth, aes('factor(Num_features)', 'factor(MaxDepth)', fill='Accuracy'))\n",
        " + geom_tile(aes(width=.95, height=.95))\n",
        " + geom_text(aes(label='Accuracy'), size=10)  # modified\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "Looking at our matrix of accuracies, What do you think? What combination of Max_Depth and Num_features would you pick?\n",
        "<br> I would pick"
      ],
      "metadata": {
        "id": "5rTbf83TXlBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer for task 3:**"
      ],
      "metadata": {
        "id": "tramFDbnYLm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "Make a list of all the Hyper parameters we have used so far for training Random Forests."
      ],
      "metadata": {
        "id": "e4VOA20mW5f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer for task 4:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sLrKi_lWXLu_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPaDEjlmoukL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}