{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pleunipennings/CSC508_ML_Biomedicine_Class/blob/main/Module5/Module_5a_Adni_BoostedTrees_PatData_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSKld7ap8iGP"
      },
      "source": [
        "# Welcome to the Gradient-Boosted Trees notebook!\n",
        "This notebook was created at San Francisco State University for the PINC and gSTAR programs by Dr Pleuni Pennings, Lucy Moctezuma Tan and Lorena Benitez Rivera. We acknowledge help from Dr Adegoke Ojewole and Dr Hector Corrada Bravo from Genentech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eypv_1yPXe2X"
      },
      "source": [
        "# Opening the file location and loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the scikit-learn version to ensure compatibility between scikit-learn and XGBoost.\n",
        "!pip install scikit-learn==1.5.2"
      ],
      "metadata": {
        "id": "V7sNzAM2DV4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F48UupL7Lt5S"
      },
      "source": [
        "# Below we are importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importing packages for Creating ML model and Evaluating it\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Importing library for plots\n",
        "from matplotlib import pyplot as plt\n",
        "from xgboost import plot_tree\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZvCupM1WwFM"
      },
      "source": [
        "Read the dataset \"PatData_cleaned.csv\", this dataset is already cleaned, we have dropped all missing values and it should only contain the main diagnoses:\n",
        "\n",
        "|Diagnosis|Meaning|\n",
        "|---|---|\n",
        "|NL|Cognitively normal|\n",
        "|MCI|Mild Cognitive Impairment|\n",
        "|Dementia|Person that has Alzheimer's Disease|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XRGmk1gGSxD"
      },
      "source": [
        "# Reading cleaned Dataset from Github\n",
        "url = \"https://raw.githubusercontent.com/pleunipennings/CSC508Data/main/PatData_cleaned.csv\"\n",
        "data = pd.read_csv(url)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6qb4QTyZJpy"
      },
      "source": [
        "# checking counts for people with each type of diagnosis\n",
        "data['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpKfJnBqwoce"
      },
      "source": [
        "##  Preparing Training Data and Creating Gradient Boosted Tree Model Object\n",
        "\n",
        "Split the data in labels (the diagnosis) and features (the other columns).  \n",
        "Every algorithm works a bit differently depending on how each package is written, which is why it is always important to be updated on changes of your more used packages. In this particular case we see that for the Gradient Boosted tree from sklearn package the labels need to be numeric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7KzBd9bit-2"
      },
      "source": [
        "# Separating labels from the general dataframe\n",
        "labels = data[\"DX\"]\n",
        "\n",
        "# Creating a label encoder object\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# Fitting the label encoder into the labels columns\n",
        "le.fit(data[\"DX\"])\n",
        "\n",
        "# Transforming the classes into numbers\n",
        "labels_t = le.transform(data[\"DX\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see our actual named Diagnosis, and then our transformed labels. As you can see now:\n",
        "- Dementia = 0\n",
        "- MCI = 1\n",
        "- NL = 2"
      ],
      "metadata": {
        "id": "2gFdMZXQV3KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classes we have\n",
        "list(le.classes_)"
      ],
      "metadata": {
        "id": "1iwB5g2vVrTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing how labels got transformed\n",
        "np.unique(labels_t)"
      ],
      "metadata": {
        "id": "EhFhctcNWKYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for our features we will drop patient ID, because it does not help us make any predictions and we drop the diagnosis since that is our label. All other columns should be considered as predictor features."
      ],
      "metadata": {
        "id": "2_Zb4CC6W0g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unnecessary columns for our features\n",
        "features = data.drop(columns=['PTID','DX'])"
      ],
      "metadata": {
        "id": "pnEC4OOfUL3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next part we should be pretty familiar with at this point:\n",
        " - We will separate our training and testing datasets using the labels and features we have been stablished.\n",
        " - We create the ML model object in this case it is Our Gradient Boosted Trees\n",
        "\n",
        "**NOTE:** Notice that I have set ahead of time a couple of hyperparameters for my Gradient Boosted Tree already, such as a **seed** (for reproducible results), **eval_metric** (metric used to measure error, in this case is: \"merror\"), **max_depth** (each tree created will have a max of 4 layers deep before getting to a leaf), **learning_rate** (How fast we want our model to learn), **n_estimators** (Number of trees created by our model per class)"
      ],
      "metadata": {
        "id": "gVMWxS1_XiJk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK-pU-gG1AH0"
      },
      "source": [
        "# As mentioned in the textbook, we use about 70-80% of our data as the training data and the rest as test data. In the code, 70% training and 30% test\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels_t, test_size=0.3, random_state=42)\n",
        "\n",
        "#Create a Gradient Boosted Tree\n",
        "gbt = XGBClassifier(seed = 42, eval_metric=\"merror\", max_depth=4, learning_rate=0.5, n_estimators=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training using validation Data Gradient Boosted Tree\n",
        "\n",
        "In order to illustrate how each tree created in the Gradient Boosted Tree improves, I will use a small validation dataset from the training data everytime our model makes a new tree.\n",
        "\n",
        "- **Validation Data:** Must come from training data because, our test set is reserved for our final evaluation of the model. We use it in our example below so that you can see behind the scenes how much each iteration of trees get better results. In our case we will use 20 percent of our training data for our validation test.\n",
        "\n",
        "**NOTE:** We should never use the test data during training because that would bias our model. This would be like knowing ahead of time the exact questions for an exam and then scoring high!"
      ],
      "metadata": {
        "id": "Ere2-smY0SDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting validation data from training data\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(features_train, labels_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Validation set\n",
        "validation_set = [(X_train,Y_train),(X_valid, Y_valid)]\n",
        "\n",
        "# Train a gradient Boosted Tree with validation data\n",
        "gbt.fit(features_train, labels_train , eval_set = validation_set)"
      ],
      "metadata": {
        "id": "gkm-thBl0THq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output above shows us 2 columns, the first shows us how our model is doing in 80% percent of our training data, whereas the the other column shows what our validation set (20% of training data) is doing at each iteration. Looking at both columns at the same time we can get an idea of whether we might be overfirring or not. Since we are using the **merror** (Multiclass classification Error Rate) as our metric, we want to see that both columns show a decrease in this metric. As you can see they both do.\n",
        "\n",
        "**NOTE:** For Multiclass Classifications the Gradient Boosted Tree Model actually creates 50 trees for each class! Dementia, MCI and NL.\n",
        "\n",
        "Below you can actually get a summary of all the trees that were created. There are a lot of trees so we wont print all of them considering there is (50 X 3 classes) 150 trees total!"
      ],
      "metadata": {
        "id": "iZYEjPENYatx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a list of all the trees created\n",
        "gbt_treelist = gbt.get_booster().get_dump()\n",
        "# Getting total amount of trees from XGboost Classifier model\n",
        "print(len(gbt_treelist))"
      ],
      "metadata": {
        "id": "KxID3aj-BNhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can for example look at the first 2 trees created. The results below shows you the index for each node it creates for both of the trees. For example the **root node [index = 0] for the first tree is [Entorhinal<3239.82129]**"
      ],
      "metadata": {
        "id": "mtvwnFuxEnVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the total amopunt of trees\n",
        "for tree in gbt_treelist[0:2]:\n",
        "  print(tree)"
      ],
      "metadata": {
        "id": "uvgytheqyAKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing one tree from our Gradient Boosted Trees Model\n",
        "\n",
        "The text summary above looks so convoluted and ugly! Below we can visualize one of our trees from our Gradient Boosted Tree Model. Lets choose in this case to look at the very first tree. You can change the index of the tree by changing the argument **num_trees**. Notice that this tree is just laying on its side, the rootnode is on the left while the leaves are towards the right."
      ],
      "metadata": {
        "id": "15AyqBnMlzaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a graph for our very first tree in Gradient Boosted Tree model\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "xgb.plot_tree(gbt, num_trees=0, ax=ax, rankdir=\"LR\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nWw8G25YmTi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: looking at the trees.\n",
        "\n",
        "1. Try plotting 2 or 3 more Decisions trees created. Why do you think they have 4 or less layers?"
      ],
      "metadata": {
        "id": "wMxEW5fsJ36A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph some trees here and answer your question\n"
      ],
      "metadata": {
        "id": "aYnUGpi2Lsfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer to task 1**"
      ],
      "metadata": {
        "id": "f8cJntUsLy8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall however that one of the biggest difference between Random Forests and Gradient Boosted Trees is that not all trees have equal amount of say on the final decision. As each tree created in Gradient Boosted Tree model tries to take into account the errors from the previous one, the trees with the lowest errors should have more say than the ones with more errors."
      ],
      "metadata": {
        "id": "_THJMJDFG67t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating our Gradient Boosted Tree\n",
        "\n",
        "Below we will finally use our **TESTING DATA** to evaluate our model. Our Testing data has never been seen before by our model, so this evaluation would emulate how our model could perform once it is deployed. Below we will use our model to make predictions four out test data and the look at the first 10 predicted values"
      ],
      "metadata": {
        "id": "ic2Nceu_jmR4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgjPJNgQjNMp"
      },
      "source": [
        "#Predict the response for test dataset\n",
        "labels_pred = gbt.predict(features_test)\n",
        "\n",
        "# Look at the predicted values.\n",
        "print(labels_pred[:10])\n",
        "#Compare with the real data from the test data set.\n",
        "print(labels_test[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way a Gradient Boosted tree predicts each of the classes is through calculating different probabilities for each class. In the output below you can see that each row constitutes one prediction. Within each prediction we see 3 numbers. If you sum all the three number you get 100%. The index with the highest number is the final label predicted. Below is an example for the first predicted label:\n",
        "\n",
        "|DX:|Dementia|Mild Cognitive Impairment|Normal|\n",
        "|---|---|---|---|\n",
        "|Index|0|1|2|\n",
        "|Probabilities|0.9004508|0.05835567|0.0411935|"
      ],
      "metadata": {
        "id": "0f1NP8s0Qntm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the probabilities predicted for each class\n",
        "preds_proba = gbt.predict_proba(features_test)\n",
        "print(preds_proba[:10])"
      ],
      "metadata": {
        "id": "t-gpSf1v4-tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2:\n",
        "What are the 10 first diagnosis predicted by our Gradient Boosted Tree?"
      ],
      "metadata": {
        "id": "P5iIflFYlHMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer for Task 2**"
      ],
      "metadata": {
        "id": "0y0Nxhnplh7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will be plotting a Confusion matrix to Check to Check how our Gradient Boosted Tree has performed. We will also be calculating it's accuracy."
      ],
      "metadata": {
        "id": "kx-uxwYCTy_N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOcKsu04jUkD"
      },
      "source": [
        "#Let's visualize how well the GBT does.\n",
        "print(metrics.confusion_matrix(labels_test, labels_pred))\n",
        "plt2 = metrics.ConfusionMatrixDisplay.from_estimator(gbt, features_test, labels_test)\n",
        "plt.grid(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZBvUOh5w_qJ"
      },
      "source": [
        "# We want to check the accuracy in predicting the test data to make sure the model is not overfitted to the training data\n",
        "accuracy = accuracy_score(labels_test, labels_pred)\n",
        "print(\"Accuracy: %.1f%%\" % (accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrHbbUmA4JwC"
      },
      "source": [
        "#Task 3: Boosted trees\n",
        "\n",
        "Now it's your turn to train a Gradient-boosted tree model and a Random Forest model, and see which one does better in terms of overall accuracy.\n",
        "\n",
        "To make sure your model is a bit different from what we did previously in this notebook, I want you to choose just two of the diagnosis categories (NL, MCI, Dementia). With just two categories, the accuracy may become better than what we had before.  \n",
        "\n",
        "NOTE: When you run the model with 2 instead of 3 categories, the eval_metric argument needs to change from the multi-class error rate (for 3 or more classes) to the binary error rate, so instead of \"merror\", you will use \"error\"\n",
        "\n",
        "1. Create your smaller dataset with just two diagnosis categories.\n",
        "2. Split label and features, split training and test.\n",
        "3. Fit your models (gradient boosted trees and random forest).\n",
        "4. Predict for your test data and calculate accuracies.\n",
        "5. Plot your results in a confusion matrix.\n",
        "6. Which of the model does better? Is it a big difference? Do you think that any hyperparameter tuning could improve them?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Random Forest Model, train and evaluate it"
      ],
      "metadata": {
        "id": "UGgtoQnFoW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gradient Boosted Tree Model, train and evaluate it"
      ],
      "metadata": {
        "id": "8_XDUX3ooURu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQS9s-FpEHlt"
      },
      "source": [
        "**Answers for task 3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blWFnBnPoKk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}